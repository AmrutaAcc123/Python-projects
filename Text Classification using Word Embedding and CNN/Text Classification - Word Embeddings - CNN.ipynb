{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification Using Word Embeddings and CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the text data\n",
    "Iterate over the folders in which our text documents are stored, and format them into a list of documents. \n",
    "At the same time also prepare a list of class indices matching the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = []          # list of text samples\n",
    "labels = []        # list of label ids\n",
    "labels_Index = {}  # dictionary mapping label index to label name\n",
    "\n",
    "PATH = os.getcwd()\n",
    "\n",
    "TEXT_DATA_DIR = os.path.join(PATH, \"txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 docs.\n"
     ]
    }
   ],
   "source": [
    "for name in os.listdir(TEXT_DATA_DIR):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_Id = len(labels_Index)\n",
    "        labels_Index[label_Id] = name\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            fpath = os.path.join(path, fname)\n",
    "            f = open(fpath, encoding = \"ISO-8859-1\")\n",
    "            t = f.read()\n",
    "            docs.append(t)\n",
    "            f.close()\n",
    "            labels.append(label_Id)\n",
    "\n",
    "print('Found %s docs.' % len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the text samples and labels into tensors that can be fed into a neural network. \n",
    "\n",
    "To do this, we will rely on Keras utilities \n",
    "\n",
    "    keras.preprocessing.text.Tokenizer \n",
    "    keras.preprocessing.sequence.pad_sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenizer__\n",
    "\n",
    "    Class for vectorizing texts, or/and turning texts into sequences (=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\n",
    "\n",
    "__fit_on_texts(texts)__\n",
    "\n",
    "    Arguments:  \n",
    "        texts: list of texts to train on.\n",
    "        \n",
    "__word_index__ attribute: \n",
    "\n",
    "    Dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7314 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Prepare tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "\n",
    "word_Index = tokenizer.word_index\n",
    "\n",
    "vocab_Size = len(word_Index) + 1\n",
    "print('Found %s unique tokens.' % vocab_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__texts_to_sequences(texts)__\n",
    "\n",
    "    Arguments:\n",
    "        texts: list of texts to turn to sequences.\n",
    "    Return: list of sequences (one per text input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition.\n",
      "\n",
      "Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents.\n",
      "\n",
      "So it has been. So it must be with this generation of Americans.\n",
      "\n",
      "That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequence of greed and irresponsibility on the part of some, but also our collective failure to make hard choices and prepare the nation for a new age. Homes have been lost; jobs shed; businesses shuttered. Our health care is too costly; our schools fail too many; and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet.\n",
      "\n",
      "These are the indicators of crisis, subject to data and statistics. Less measurable but no less profound is a sapping of confidence across our land - a nagging fear that America's decline is inevitable, and that the next generation must lower its sights.\n",
      "\n",
      "Today I say to you that the challenges we face are real. They are serious and they are many. They will not be met easily or in a short span of time. But know this, America - they will be met.\n",
      "\n",
      "On this day, we gather because we have chosen hope over fear, unity of purpose over conflict and discord.\n",
      "\n",
      "On this day, we come to proclaim an end to the petty grievances and false promises, the recriminations and worn out dogmas, that for far too long have strangled our politics.\n",
      "\n",
      "We remain a young nation, but in the words of Scripture, the time has come to set aside childish things. The time has come to reaffirm our enduring spirit; to choose our better history; to carry forward that precious gift, that noble idea, passed on from generation to generation: the God-given promise that all are equal, all are free, and all deserve a chance to pursue their full measure of happiness.\n",
      "\n",
      "In reaffirming the greatness of our nation, we understand that greatness is never a given. It must be earned. Our journey has never been one of short-cuts or settling for less. It has not been the path for the faint-hearted - for those who prefer leisure over work, or seek only the pleasures of riches and fame. Rather, it has been the risk-takers, the doers, the makers of things - some celebrated but more often men and women obscure in their labor, who have carried us up the long, rugged path towards prosperity and freedom.\n",
      "\n",
      "For us, they packed up their few worldly possessions and traveled across oceans in search of a new life.\n",
      "\n",
      "For us, they toiled in sweatshops and settled the West; endured the lash of the whip and plowed the hard earth.\n",
      "\n",
      "For us, they fought and died, in places like Concord and Gettysburg; Normandy and Khe Sahn.\n",
      "\n",
      "Time and again these men and women struggled and sacrificed and worked till their hands were raw so that we might live a better life. They saw America as bigger than the sum of our individual ambitions; greater than all the differences of birth or wealth or faction.\n",
      "\n",
      "This is the journey we continue today. We remain the most prosperous, powerful nation on Earth. Our workers are no less productive than when this crisis began. Our minds are no less inventive, our goods and services no less needed than they were last week or last month or last year. Our capacity remains undiminished. But our time of standing pat, of protecting narrow interests and putting off unpleasant decisions - that time has surely passed. Starting today, we must pick ourselves up, dust ourselves off, and begin again the work of remaking America.\n",
      "\n",
      "For everywhere we look, there is work to be done. The state of the economy calls for action, bold and swift, and we will act - not only to create new jobs, but to lay a new foundation for growth. We will build the roads and bridges, the electric grids and digital lines that feed our commerce and bind us together. We will restore science to its rightful place, and wield technology's wonders to raise health care's quality and lower its cost. We will harness the sun and the winds and the soil to fuel our cars and run our factories. And we will transform our schools and colleges and universities to meet the demands of a new age. All this we can do. And all this we will do.\n",
      "\n",
      "Now, there are some who question the scale of our ambitions - who suggest that our system cannot tolerate too many big plans. Their memories are short. For they have forgotten what this country has already done; what free men and women can achieve when imagination is joined to common purpose, and necessity to courage.\n",
      "\n",
      "What the cynics fail to understand is that the ground has shifted beneath them - that the stale political arguments that have consumed us for so long no longer apply. The question we ask today is not whether our government is too big or too small, but whether it works - whether it helps families find jobs at a decent wage, care they can afford, a retirement that is dignified. Where the answer is yes, we intend to move forward. Where the answer is no, programs will end. And those of us who manage the public's dollars will be held to account - to spend wisely, reform bad habits, and do our business in the light of day - because only then can we restore the vital trust between a people and their government.\n",
      "Nor is the question before us whether the market is a force for good or ill. Its power to generate wealth and expand freedom is unmatched, but this crisis has reminded us that without a watchful eye, the market can spin out of control - and that a nation cannot prosper long when it favors only the prosperous. The success of our economy has always depended not just on the size of our Gross Domestic Product, but on the reach of our prosperity; on the ability to extend opportunity to every willing heart - not out of charity, but because it is the surest route to our common good.\n",
      "\n",
      "As for our common defense, we reject as false the choice between our safety and our ideals. Our Founding Fathers, faced with perils we can scarcely imagine, drafted a charter to assure the rule of law and the rights of man, a charter expanded by the blood of generations. Those ideals still light the world, and we will not give them up for expedience's sake. And so to all other peoples and governments who are watching today, from the grandest capitals to the small village where my father was born: know that America is a friend of each nation and every man, woman, and child who seeks a future of peace and dignity, and we are ready to lead once more.\n",
      "\n",
      "Recall that earlier generations faced down fascism and communism not just with missiles and tanks, but with the sturdy alliances and enduring convictions. They understood that our power alone cannot protect us, nor does it entitle us to do as we please. Instead, they knew that our power grows through its prudent use; our security emanates from the justness of our cause, the force of our example, the tempering qualities of humility and restraint.\n",
      "\n",
      "We are the keepers of this legacy. Guided by these principles once more, we can meet those new threats that demand even greater effort - even greater cooperation and understanding between nations. We will begin to responsibly leave Iraq to its people, and forge a hard-earned peace in Afghanistan. With old friends and former foes, weÂ’ll work tirelessly to lessen the nuclear threat, and roll back the specter of a warming planet. We will not apologize for our way of life, nor will we waver in its defense, and for those who seek to advance their aims by inducing terror and slaughtering innocents, we say to you now that our spirit is stronger and cannot be broken; you cannot outlast us, and we will defeat you.\n",
      "\n",
      "For we know that our patchwork heritage is a strength, not a weakness. We are a nation of Christians and Muslims, Jews and Hindus - and non-believers. We are shaped by every language and culture, drawn from every end of this Earth; and because we have tasted the bitter swill of civil war and segregation, and emerged from that dark chapter stronger and more united, we cannot help but believe that the old hatreds shall someday pass; that the lines of tribe shall soon dissolve; that as the world grows smaller, our common humanity shall reveal itself; and that America must play its role in ushering in a new era of peace.\n",
      "\n",
      "To the Muslim world, we seek a new way forward, based on mutual interest and mutual respect. To those leaders around the globe who seek to sow conflict, or blame their society's ills on the West - know that your people will judge you on what you can build, not what you destroy. To those who cling to power through corruption and deceit and the silencing of dissent, know that you are on the wrong side of history; but that we will extend a hand if you are willing to unclench your fist.\n",
      "\n",
      "To the people of poor nations, we pledge to work alongside you to make your farms flourish and let clean waters flow; to nourish starved bodies and feed hungry minds. And to those nations like ours that enjoy relative plenty, we say we can no longer afford indifference to the suffering outside our borders; nor can we consume the world's resources without regard to effect. For the world has changed, and we must change with it.\n",
      "\n",
      "As we consider the road that unfolds before us, we remember with humble gratitude those brave Americans who, at this very hour, patrol far-off deserts and distant mountains. They have something to tell us, just as the fallen heroes who lie in Arlington whisper through the ages. We honor them not only because they are guardians of our liberty, but because they embody the spirit of service; a willingness to find meaning in something greater than themselves. And yet, at this moment - a moment that will define a generation - it is precisely this spirit that must inhabit us all.\n",
      "\n",
      "For as much as government can do and must do, it is ultimately the faith and determination of the American people upon which this nation relies. It is the kindness to take in a stranger when the levees break, the selflessness of workers who would rather cut their hours than see a friend lose their job which sees us through our darkest hours. It is the firefighter's courage to storm a stairway filled with smoke, but also a parent's willingness to nurture a child, that finally decides our fate.\n",
      "\n",
      "Our challenges may be new. The instruments with which we meet them may be new. But those values upon which our success depends - honesty and hard work, courage and fair play, tolerance and curiosity, loyalty and patriotism - these things are old. These things are true. They have been the quiet force of progress throughout our history. What is demanded then is a return to these truths. What is required of us now is a new era of responsibility - a recognition, on the part of every American, that we have duties to ourselves, our nation, and the world, duties that we do not grudgingly accept but rather seize gladly, firm in the knowledge that there is nothing so satisfying to the spirit, so defining of our character, than giving our all to a difficult task.\n",
      "\n",
      "This is the price and the promise of citizenship.\n",
      "\n",
      "This is the source of our confidence - the knowledge that God calls on us to shape an uncertain destiny.\n",
      "\n",
      "This is the meaning of our liberty and our creed - why men and women and children of every race and every faith can join in celebration across this magnificent mall, and why a man whose father less than sixty years ago might not have been served at a local restaurant can now stand before you to take a most sacred oath.\n",
      "\n",
      "So let us mark this day with remembrance, of who we are and how far we have traveled. In the year of America's birth, in the coldest of months, a small band of patriots huddled by dying campfires on the shores of an icy river. The capital was abandoned. The enemy was advancing. The snow was stained with blood. At a moment when the outcome of our revolution was most in doubt, the father of our nation ordered these words be read to the people:\n",
      "\"Let it be told to the future world...that in the depth of winter, when nothing but hope and virtue could survive...that the city and the country, alarmed at one common danger, came forth to meet [it].\"\n",
      "\n",
      "America. In the face of our common dangers, in this winter of our hardship, let us remember these timeless words. With hope and virtue, let us brave once more the icy currents, and endure what storms may come. Let it be said by our children's children that when we were tested we refused to let this journey end, that we did not turn back nor did we falter; and with eyes fixed on the horizon and God's grace upon us, we carried forth that great gift of freedom and delivered it safely to future generations.\n",
      "\n",
      "Thank you. God bless you and God bless the United States of America.\n",
      "  [8, 272, 143, 107, 2325, 25, 1, 954, 191, 42, 1422, 9, 1, 719, 13, 18, 4232, 4233, 4, 1, 1894, 2991, 25, 12, 4234, 8, 159, 48, 562, 9, 49, 508, 3, 12, 51, 23, 281, 23, 1, 2326, 2, 1165, 30, 31, 2327, 957, 16, 2328, 2329, 125, 100, 18, 96, 769, 1, 1915, 964, 1, 443, 18, 68, 1418, 253, 1661, 4235, 4, 356, 2, 1, 198, 2330, 4, 374, 624, 78, 47, 563, 1, 964, 10, 769, 2992, 1299, 4236, 2, 2993, 2994, 38, 73, 1300, 36, 31, 1160, 19, 24, 1441, 64, 4, 1, 2331, 39, 321, 4, 86, 6, 245, 383, 28, 64, 11, 1, 46, 18, 4237, 2332, 3, 1, 818, 4, 12, 2995, 2, 717, 3, 12, 1442, 4238, 47, 15, 31, 68, 47, 15, 97, 21, 17, 16, 345, 4, 100, 7, 11, 20, 6, 1, 965, 4, 1038, 10, 96, 281, 1662, 12, 51, 10, 38, 178, 333, 5, 310, 2333, 626, 4, 1050, 2, 2334, 12, 145, 10, 2996, 4239, 5, 2335, 4, 2336, 2, 2997, 19, 1, 444, 4, 164, 28, 99, 12, 2998, 819, 3, 101, 225, 1443, 2, 2337, 1, 51, 9, 5, 61, 720, 581, 18, 68, 486, 90, 4240, 466, 2999, 12, 237, 188, 10, 227, 3000, 12, 155, 1916, 227, 76, 2, 208, 120, 1301, 675, 3001, 7, 1, 1166, 11, 291, 140, 1444, 12, 3002, 2, 2338, 12, 536, 73, 20, 1, 4241, 4, 1038, 1445, 3, 564, 2, 1917, 231, 4242, 28, 110, 231, 1167, 10, 5, 4243, 4, 401, 163, 12, 398, 5, 4244, 958, 7, 1168, 3003, 10, 3004, 2, 7, 1, 220, 345, 97, 1663, 138, 2339, 107, 8, 172, 3, 13, 7, 1, 263, 11, 353, 20, 292, 35, 20, 1302, 2, 35, 20, 76, 35, 14, 24, 21, 244, 4245, 39, 6, 5, 676, 4246, 4, 57, 28, 81, 16, 36, 35, 14, 21, 244, 19, 16, 120, 11, 3005, 64, 11, 18, 2340, 181, 147, 958, 2315, 4, 588, 147, 1169, 2, 3006, 19, 16, 120, 11, 114, 3, 4247, 43, 309, 3, 1, 1664, 4248, 2, 1290, 677, 1, 3007, 2, 1918, 74, 4249, 7, 9, 310, 227, 215, 18, 4250, 12, 330, 11, 1051, 5, 104, 51, 28, 6, 1, 443, 4, 3008, 1, 57, 31, 114, 3, 627, 1446, 4251, 229, 1, 57, 31, 114, 3, 1660, 12, 1435, 385, 3, 589, 12, 92, 230, 3, 2341, 758, 7, 3009, 1170, 7, 1665, 966, 1052, 19, 27, 345, 3, 345, 1, 195, 334, 187, 7, 32, 20, 967, 32, 20, 179, 2, 32, 467, 5, 462, 3, 820, 34, 468, 712, 4, 1303, 6, 4252, 1, 960, 4, 12, 51, 11, 331, 7, 960, 10, 152, 5, 334, 15, 97, 21, 1034, 12, 577, 31, 152, 68, 41, 4, 676, 1171, 39, 4253, 9, 231, 15, 31, 24, 68, 1, 469, 9, 1, 3010, 4254, 9, 86, 22, 4255, 4256, 147, 50, 39, 763, 128, 1, 4257, 4, 3011, 2, 4258, 897, 15, 31, 68, 1, 510, 4259, 1, 4260, 1, 3012, 4, 229, 164, 1919, 28, 45, 563, 210, 2, 189, 4261, 6, 34, 3013, 22, 18, 1160, 42, 62, 1, 215, 4262, 469, 430, 356, 2, 254, 9, 42, 35, 4263, 62, 34, 282, 4264, 2342, 2, 1172, 163, 1920, 6, 1447, 4, 5, 61, 85, 9, 42, 35, 4265, 6, 4266, 2, 4267, 1, 1173, 2939, 1, 4268, 4, 1, 4269, 2, 4270, 1, 225, 226, 9, 42, 35, 619, 2, 1174, 6, 1053, 63, 2951, 2, 3014, 4271, 2, 4272, 4273, 57, 2, 274, 73, 210, 2, 189, 3015, 2, 1288, 2, 402, 2343, 34, 814, 95, 4274, 47, 7, 11, 428, 465, 5, 92, 85, 35, 399, 36, 23, 1448, 77, 1, 3016, 4, 12, 622, 2344, 487, 77, 32, 1, 968, 4, 969, 39, 1047, 39, 4275, 16, 10, 1, 577, 11, 721, 107, 11, 1051, 1, 168, 628, 1054, 51, 19, 226, 12, 388, 20, 110, 231, 1921, 77, 40, 16, 1038, 664, 12, 416, 20, 110, 231, 4276, 12, 3017, 2, 821, 110, 231, 722, 77, 35, 95, 139, 1055, 39, 139, 1056, 39, 139, 190, 12, 723, 3018, 4277, 28, 12, 57, 4, 886, 4278, 4, 2345, 3019, 970, 2, 1304, 329, 4279, 1666, 7, 57, 31, 1922, 1052, 1923, 107, 11, 97, 1924, 427, 62, 1654, 427, 329, 2, 620, 274, 1, 50, 4, 2965, 36, 9, 1449, 11, 200, 59, 10, 50, 3, 21, 369, 1, 162, 4, 1, 145, 629, 9, 403, 1305, 2, 4280, 2, 11, 14, 678, 24, 128, 3, 293, 61, 90, 28, 3, 1667, 5, 61, 445, 9, 283, 11, 14, 204, 1, 3020, 2, 3021, 1, 898, 4281, 2, 3022, 881, 7, 3023, 12, 2346, 2, 4282, 42, 142, 11, 14, 542, 44, 3, 138, 3024, 249, 2, 4283, 4284, 2296, 3, 770, 237, 4285, 446, 2, 1663, 138, 357, 11, 14, 1900, 1, 543, 2, 1, 2347, 2, 1, 1175, 3, 1057, 12, 1048, 2, 771, 12, 4286, 2, 11, 14, 772, 12, 155, 2, 2348, 2, 1668, 3, 332, 1, 1306, 4, 5, 61, 720, 32, 16, 11, 29, 52, 2, 32, 16, 11, 14, 52, 96, 59, 20, 164, 22, 265, 1, 1046, 4, 12, 2344, 22, 773, 7, 12, 103, 218, 3025, 227, 76, 238, 899, 34, 3026, 20, 676, 9, 35, 18, 1906, 37, 16, 71, 31, 539, 369, 37, 179, 210, 2, 189, 29, 271, 40, 1659, 10, 1669, 3, 400, 588, 2, 2349, 3, 417, 37, 1, 3027, 1916, 3, 331, 10, 7, 1, 822, 31, 4287, 4288, 84, 7, 1, 2350, 470, 4289, 7, 18, 4290, 42, 9, 47, 215, 110, 710, 4291, 1, 265, 11, 384, 107, 10, 24, 404, 12, 130, 10, 227, 238, 39, 227, 135, 28, 404, 15, 774, 404, 15, 1176, 153, 312, 90, 38, 5, 1450, 1451, 188, 35, 29, 389, 5, 971, 7, 10, 4292, 75, 1, 458, 10, 464, 11, 1670, 3, 972, 758, 75, 1, 458, 10, 110, 973, 14, 309, 2, 86, 4, 42, 22, 1452, 1, 4293, 535, 14, 21, 1161, 3, 2351, 3, 1177, 4294, 823, 775, 3028, 2, 52, 12, 134, 6, 1, 544, 4, 120, 64, 128, 154, 29, 11, 542, 1, 900, 719, 266, 5, 46, 2, 34, 130, 824, 10, 1, 265, 191, 42, 404, 1, 901, 10, 5, 1307, 9, 123, 39, 2352, 138, 224, 3, 1178, 1047, 2, 1671, 254, 10, 4295, 28, 16, 1038, 31, 1672, 42, 7, 243, 5, 4296, 1673, 1, 901, 29, 2353, 74, 4, 1179, 2, 7, 5, 51, 218, 3029, 215, 40, 15, 4297, 128, 1, 628, 1, 239, 4, 12, 145, 31, 214, 3030, 24, 82, 19, 1, 974, 4, 12, 4298, 1925, 630, 28, 19, 1, 561, 4, 12, 356, 19, 1, 724, 3, 1926, 311, 3, 78, 447, 460, 24, 74, 4, 1927, 28, 64, 15, 10, 1, 4299, 2354, 3, 12, 400, 123, 23, 9, 12, 400, 631, 11, 4300, 23, 1290, 1, 358, 266, 12, 3031, 2, 12, 818, 12, 1442, 1641, 1308, 17, 4301, 11, 29, 4302, 1032, 4303, 5, 3032, 3, 2355, 1, 2356, 4, 590, 2, 1, 825, 4, 251, 5, 3032, 3033, 25, 1, 2357, 4, 718, 86, 818, 198, 544, 1, 58, 2, 11, 14, 24, 144, 84, 62, 9, 4304, 1928, 2, 47, 3, 32, 113, 3034, 2, 2358, 22, 20, 1155, 107, 27, 1, 4305, 3035, 3, 1, 135, 591, 75, 26, 346, 33, 182, 81, 7, 36, 10, 5, 887, 4, 208, 51, 2, 78, 251, 541, 2, 267, 22, 4306, 5, 116, 4, 374, 2, 826, 2, 11, 20, 322, 3, 165, 366, 45, 1180, 7, 1929, 718, 1308, 171, 3036, 2, 4307, 24, 82, 17, 4308, 2, 2359, 28, 17, 1, 4309, 2313, 2, 1435, 4310, 35, 1662, 7, 12, 224, 714, 218, 512, 42, 824, 418, 15, 4311, 42, 3, 52, 23, 11, 1930, 545, 35, 827, 7, 12, 224, 1931, 102, 138, 4312, 291, 12, 319, 2360, 27, 1, 4313, 4, 12, 579, 1, 1307, 4, 12, 632, 1, 4314, 1674, 4, 1295, 2, 4315, 11, 20, 1, 4316, 4, 16, 1453, 2361, 25, 73, 828, 366, 45, 11, 29, 332, 86, 61, 892, 7, 1058, 158, 487, 1059, 158, 487, 1165, 2, 725, 266, 335, 11, 14, 620, 3, 3037, 565, 371, 3, 138, 46, 2, 3038, 5, 225, 1034, 374, 6, 583, 17, 426, 80, 2, 2362, 4317, 829, 50, 2363, 3, 4318, 1, 419, 1454, 2, 2364, 129, 1, 4319, 4, 5, 3039, 536, 11, 14, 24, 1675, 9, 12, 141, 4, 85, 824, 14, 11, 4320, 6, 138, 631, 2, 9, 86, 22, 763, 3, 2365, 34, 3040, 25, 4321, 1455, 2, 4322, 3041, 11, 172, 3, 13, 96, 7, 12, 385, 10, 1060, 2, 218, 21, 975, 13, 218, 4323, 42, 2, 11, 14, 586, 13, 9, 11, 81, 7, 12, 4324, 1061, 10, 5, 580, 24, 5, 3042, 11, 20, 5, 51, 4, 3043, 2, 3044, 4325, 2, 4326, 2, 976, 4327, 11, 20, 1309, 25, 78, 1932, 2, 830, 4328, 27, 78, 309, 4, 16, 226, 2, 64, 11, 18, 4329, 1, 1897, 4330, 4, 1181, 178, 2, 4331, 2, 1933, 27, 7, 2366, 1456, 1060, 2, 45, 156, 11, 218, 170, 28, 160, 7, 1, 426, 4332, 1657, 1934, 1182, 7, 1, 881, 4, 4333, 1657, 1457, 4334, 7, 23, 1, 58, 1931, 2367, 12, 400, 1310, 1657, 3045, 959, 2, 7, 36, 97, 726, 138, 471, 6, 4335, 6, 5, 61, 1935, 4, 374, 3, 1, 1936, 58, 11, 763, 5, 61, 141, 758, 275, 19, 1676, 1937, 2, 1676, 546, 3, 86, 390, 235, 1, 1938, 22, 763, 3, 4336, 1169, 39, 2368, 34, 4337, 4338, 19, 1, 1173, 81, 7, 55, 46, 14, 4339, 13, 19, 37, 13, 29, 204, 24, 37, 13, 3046, 3, 86, 22, 4340, 3, 224, 102, 977, 2, 4341, 2, 1, 4342, 4, 3047, 81, 7, 13, 20, 19, 1, 831, 431, 4, 230, 28, 7, 11, 14, 1926, 5, 713, 69, 13, 20, 447, 3, 4343, 55, 4344, 3, 1, 46, 4, 812, 335, 11, 1939, 3, 50, 3048, 13, 3, 101, 55, 1677, 3049, 2, 112, 472, 2330, 3050, 3, 4345, 4346, 2369, 2, 3023, 2370, 416, 2, 3, 86, 335, 63, 1296, 7, 1940, 3051, 3052, 11, 172, 11, 29, 110, 710, 389, 4347, 3, 1, 1311, 2371, 12, 1941, 824, 29, 11, 4348, 1, 4349, 432, 243, 4350, 3, 1183, 9, 1, 58, 31, 1062, 2, 11, 97, 87, 17, 15, 23, 11, 1942, 1, 760, 7, 4351, 191, 42, 11, 373, 17, 4352, 1184, 86, 885, 100, 22, 38, 16, 197, 776, 4353, 310, 329, 2960, 2, 2372, 2310, 35, 18, 313, 3, 298, 42, 82, 23, 1, 978, 2373, 22, 2312, 6, 4354, 4355, 102, 1, 3053, 11, 488, 84, 24, 128, 64, 35, 20, 2374, 4, 12, 623, 28, 64, 35, 4356, 1, 385, 4, 508, 5, 1943, 3, 312, 3054, 6, 313, 487, 77, 777, 2, 624, 38, 16, 260, 5, 260, 7, 14, 1678, 5, 345, 15, 10, 3055, 16, 385, 7, 97, 3056, 42, 32, 9, 23, 211, 23, 130, 29, 52, 2, 97, 52, 15, 10, 1458, 1, 448, 2, 2974, 4, 1, 70, 46, 902, 66, 16, 51, 4357, 15, 10, 1, 2375, 3, 124, 6, 5, 3057, 40, 1, 4358, 668, 1, 3058, 4, 388, 22, 56, 897, 473, 34, 662, 77, 183, 5, 887, 1679, 34, 166, 66, 2376, 42, 102, 12, 2321, 662, 15, 10, 1, 4359, 417, 3, 4360, 5, 4361, 1185, 17, 3059, 28, 99, 5, 3060, 1943, 3, 1680, 5, 267, 7, 347, 4362, 12, 4363, 12, 263, 127, 21, 61, 1, 1944, 17, 66, 11, 332, 84, 127, 21, 61, 28, 86, 414, 902, 66, 12, 239, 1063, 3061, 2, 225, 50, 417, 2, 903, 726, 4364, 2, 1312, 3062, 2, 1293, 73, 229, 20, 426, 73, 229, 20, 717, 35, 18, 68, 1, 3063, 1307, 4, 584, 957, 12, 230, 37, 10, 3064, 154, 10, 5, 1313, 3, 73, 1945, 37, 10, 1064, 4, 42, 96, 10, 5, 61, 1935, 4, 372, 5, 4365, 19, 1, 444, 4, 78, 70, 7, 11, 18, 3065, 3, 427, 12, 51, 2, 1, 58, 3065, 7, 11, 52, 24, 4366, 1946, 28, 897, 3066, 4367, 3067, 6, 1, 205, 7, 59, 10, 679, 47, 4368, 3, 1, 385, 47, 1031, 4, 12, 1186, 77, 727, 12, 32, 3, 5, 592, 954, 16, 10, 1, 1947, 2, 1, 187, 4, 4369, 16, 10, 1, 4370, 4, 12, 401, 1, 205, 7, 195, 629, 19, 42, 3, 593, 43, 1681, 1045, 16, 10, 1, 3054, 4, 12, 623, 2, 12, 1653, 228, 210, 2, 189, 2, 93, 4, 78, 1314, 2, 78, 448, 29, 511, 6, 4371, 163, 16, 4372, 3068, 2, 228, 5, 251, 1162, 346, 231, 77, 4373, 65, 397, 428, 24, 18, 68, 832, 38, 5, 1187, 4374, 29, 96, 272, 191, 13, 3, 124, 5, 168, 3069, 964, 47, 112, 42, 1948, 16, 120, 17, 4375, 4, 22, 11, 20, 2, 67, 310, 11, 18, 1172, 6, 1, 190, 4, 1168, 969, 6, 1, 4376, 4, 482, 5, 135, 2377, 4, 1315, 2976, 25, 3070, 4377, 19, 1, 1433, 4, 43, 3071, 1316, 1, 979, 33, 1949, 1, 2378, 33, 4378, 1, 4379, 33, 4380, 17, 2357, 38, 5, 260, 40, 1, 4381, 4, 12, 833, 33, 168, 6, 1164, 1, 346, 4, 12, 51, 4382, 73, 443, 21, 1682, 3, 1, 46, 112, 15, 21, 342, 3, 1, 116, 58, 7, 6, 1, 1950, 4, 1646, 40, 679, 28, 181, 2, 3072, 173, 4383, 7, 1, 594, 2, 1, 71, 4384, 38, 41, 400, 1683, 320, 1951, 3, 332, 15, 36, 6, 1, 353, 4, 12, 400, 4385, 6, 16, 1646, 4, 12, 3073, 112, 42, 373, 73, 2324, 443, 17, 181, 2, 3072, 112, 42, 885, 366, 45, 1, 3071, 4386, 2, 1317, 37, 2994, 127, 114, 112, 15, 21, 177, 25, 12, 1952, 93, 7, 40, 11, 95, 4387, 11, 4388, 3, 112, 16, 577, 309, 7, 11, 242, 24, 449, 129, 824, 242, 11, 4389, 2, 17, 1459, 3074, 19, 1, 1460, 2, 2379, 3075, 902, 42, 11, 1160, 1951, 7, 91, 1170, 4, 254, 2, 2380, 15, 3076, 3, 116, 718, 159, 13, 195, 273, 13, 2, 195, 273, 1, 156, 136, 4, 36]\n",
      "1981\n",
      "2398\n",
      "1877\n",
      "4676\n",
      "2484\n",
      "2382\n",
      "812\n",
      "2168\n",
      "1884\n",
      "1582\n",
      "1742\n",
      "830\n",
      "3000\n",
      "3185\n",
      "2956\n",
      "4123\n",
      "1201\n",
      "2950\n",
      "3153\n",
      "453\n",
      "3049\n",
      "289\n",
      "2510\n",
      "1025\n",
      "677\n",
      "2486\n",
      "2690\n",
      "3411\n",
      "2203\n",
      "2734\n",
      "2315\n",
      "4110\n",
      "1879\n",
      "279\n",
      "809\n",
      "1141\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "print(docs[1], sequences[1])\n",
    "\n",
    "for i in sequences:\n",
    "    print (len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We will pad all input sequences to have the length of 1000. Again, we can do this with a built in Keras's pad_sequences() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (36, 1000)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a test set\n",
    "X_train = np.append(data[:10], data[12:22], axis=0)\n",
    "X_train = np.append(X_train, data[24:34], axis=0)\n",
    "\n",
    "X_test = np.append(data[10:12], data[22:24], axis=0)\n",
    "X_test = np.append(X_test, data[34:], axis=0)\n",
    "\n",
    "y_train = np.append(labels[:10], labels[12:22], axis=0)\n",
    "y_train = np.append(y_train, labels[24:34], axis=0)\n",
    "\n",
    "y_test = np.append(labels[10:12], labels[22:24],axis=0)\n",
    "y_test = np.append(y_test, labels[34:],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing the Embedding layer\n",
    "\n",
    "Compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(PATH, 'glove.6B.50d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7314, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding_Matrix = np.zeros((vocab_Size, 50))\n",
    "for word, i in word_Index.items():\n",
    "    embedding_Vector = embeddings_index.get(word)\n",
    "    if embedding_Vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_Matrix[i] = embedding_Vector\n",
    "\n",
    "print (embedding_Matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create the embedding layer\n",
    "\n",
    "The key difference is that the embedding layer can be seeded with the GloVe word embedding weights. \n",
    "\n",
    "    We chose the 50-dimensional version, therefore the Embedding layer must be defined with output_dim set to 50. \n",
    "    We do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_Size,\n",
    "                            50,\n",
    "                            weights=[embedding_Matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally build a small 1D convnet to solve our classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(4)(x)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(4)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "preds = Dense(len(labels_Index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 50)          365700    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 64)           16064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 245, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 57, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 14, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                57408     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 480,455\n",
      "Trainable params: 114,755\n",
      "Non-trainable params: 365,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 1.1334 - acc: 0.3667\n",
      "Epoch 2/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.9778 - acc: 0.3333\n",
      "Epoch 3/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.3163 - acc: 0.3333\n",
      "Epoch 4/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0841 - acc: 0.3667\n",
      "Epoch 5/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.9650 - acc: 0.3333\n",
      "Epoch 6/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8881 - acc: 0.9000\n",
      "Epoch 7/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.8962 - acc: 0.3667\n",
      "Epoch 8/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.0029 - acc: 0.4000\n",
      "Epoch 9/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7668 - acc: 0.7333\n",
      "Epoch 10/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.5745 - acc: 1.0000\n",
      "Epoch 11/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.4821 - acc: 0.8667\n",
      "Epoch 12/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6323 - acc: 0.6000\n",
      "Epoch 13/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.7604 - acc: 0.5333\n",
      "Epoch 14/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 1.2258 - acc: 0.4000\n",
      "Epoch 15/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.6184 - acc: 0.7000\n",
      "Epoch 16/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.3924 - acc: 1.0000\n",
      "Epoch 17/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.2794 - acc: 1.0000\n",
      "Epoch 18/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.2176 - acc: 1.0000\n",
      "Epoch 19/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1829 - acc: 1.0000\n",
      "Epoch 20/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1926 - acc: 1.0000\n",
      "Epoch 21/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1751 - acc: 1.0000\n",
      "Epoch 22/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.2462 - acc: 0.9667\n",
      "Epoch 23/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1875 - acc: 1.0000\n",
      "Epoch 24/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1622 - acc: 1.0000\n",
      "Epoch 25/25\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.0701 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ec5d60cc0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.84911704e-01   1.23765849e-01   2.91322559e-01]\n",
      " [  4.75651026e-01   1.95072576e-01   3.29276413e-01]\n",
      " [  1.34671293e-03   9.84617352e-01   1.40358750e-02]\n",
      " [  7.57883477e-04   9.94684517e-01   4.55762073e-03]\n",
      " [  3.93927187e-01   3.76961708e-01   2.29111016e-01]\n",
      " [  3.58260930e-01   7.78342336e-02   5.63904881e-01]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "y_pred =[]\n",
    "for i in Y_pred:\n",
    "    y_pred.append(np.argmax(i))\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83333333333333337"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 2, 0],\n",
       "       [1, 0, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:\n",
    "    \n",
    "    https://keras.io\n",
    "        \n",
    "    https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
